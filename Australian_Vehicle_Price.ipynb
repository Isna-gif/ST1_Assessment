{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isna-gif/ST1_Assessment/blob/main/Australian_Vehicle_Price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YvXYZ-lkr-OJ",
        "outputId": "ce660341-ca3f-4f5a-a89e-f6d445ba93bf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Australian Vehicle Prices(Australian Vehicle Prices).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2f914281a07b>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Reading the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Australian Vehicle Prices(Australian Vehicle Prices).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Display all columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Australian Vehicle Prices(Australian Vehicle Prices).csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  # For advanced visualization styles\n",
        "from scipy.stats import skew, kurtosis  # For skewness and kurtosis\n",
        "import plotly.express as px\n",
        "import matplotlib.ticker as mtick\n",
        "from scipy import stats\n",
        "from scipy.stats import f_oneway\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import warnings\n",
        "import joblib\n",
        "import tkinter as tk\n",
        "from tkinter import messagebox\n",
        "\n",
        "\n",
        "\n",
        "# Define the ticker (not used in the current context but available for future use)\n",
        "ticker = 'USIX'\n",
        "\n",
        "# Reading the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Australian Vehicle Prices(Australian Vehicle Prices).csv')\n",
        "\n",
        "# Display all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Step 2: Problem Definition\n",
        "print(\"\\n### Defining the Problem ###\")\n",
        "print(\"In this project, we aim to predict vehicle prices based on various factors.\")\n",
        "\n",
        "# Step 3: Target variable\n",
        "print(\"\\n### Target Variable ###\")\n",
        "print(\"Dependent Variable (Target): Price of vehicles\")\n",
        "print(\"Independent Variables: Kilometres, FuelConsumption, Year, Brand, Model, Car/Suv, Title, \\\n",
        "UsedOrNew, Transmission, Engine, DriveType, FuelType, ColourExtInt, Location, BodyType, \\\n",
        "CylindersinEngine, Doors, Seats\")\n",
        "\n",
        "# Ensure that the 'Price' column contains only numeric values and drop non-numeric values or NaNs\n",
        "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors='coerce')  # Convert to numeric, invalid parsing will be set as NaN\n",
        "df = df.dropna(subset=[\"Price\"])  # Drop rows where 'Price' is NaN\n",
        "\n",
        "# Plot: Distribution of 'Price'\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.hist(df[\"Price\"], bins=40, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "sns.kdeplot(df[\"Price\"], color='darkorange', lw=2)\n",
        "plt.title(\"Figure 1: Distribution of Vehicle Prices\", fontsize=16)\n",
        "plt.xlabel(\"Price (in thousands)\", fontsize=14)\n",
        "plt.ylabel(\"Frequency\", fontsize=14)\n",
        "plt.xlim(0, 200000)\n",
        "plt.ylim(0, 14000)\n",
        "plt.xticks(ticks=range(0, 200001, 10000), labels=[f'{i // 1000}K' for i in range(0, 200001, 10000)])\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Analyze skewness and kurtosis of Price\n",
        "print(\"\\n### Analyzing Distribution for Class Imbalance ###\")\n",
        "\n",
        "# Calculate skewness and kurtosis using the cleaned DataFrame 'df'\n",
        "price_skew = df[\"Price\"].skew()\n",
        "price_kurtosis = df[\"Price\"].kurt()\n",
        "print(f'Skewness = {round(price_skew, 2)}')\n",
        "print(f'Kurtosis = {round(price_kurtosis, 2)}')\n",
        "\n",
        "# Interpretation of skewness and kurtosis\n",
        "if price_skew > 0:\n",
        "    print(f\"A skewness of {round(price_skew, 2)} indicates positive skew (longer tail on the right).\")\n",
        "else:\n",
        "    print(f\"A skewness of {round(price_skew, 2)} indicates negative skew (longer tail on the left).\")\n",
        "\n",
        "if price_kurtosis > 3:\n",
        "    print(f\"A kurtosis of {round(price_kurtosis, 2)} indicates a high degree of peakedness, suggesting many outliers.\")\n",
        "else:\n",
        "    print(f\"A kurtosis of {round(price_kurtosis, 2)} is within normal range.\")\n",
        "\n",
        "#Step 4: Data exploration at basic level\n",
        "print(\"\\n### Data Exploration At Basic Level ###\")\n",
        "\n",
        "# Display original DataFrame (first 5 rows)\n",
        "print(\"\\nOriginal DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Dataset size and shape\n",
        "print(\"\\nOriginal Dataset\")\n",
        "print(f\"Size: {df.size} elements\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "\n",
        "# Display column names and data types\n",
        "print(f\"Attributes (Column Headers): {df.columns.tolist()}\")\n",
        "print(\"\\nData types before conversion:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Missing values check\n",
        "print(\"\\nMissing values before conversion:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Data cleaning: remove duplicates, convert Price and Kilometres to numeric, and remove NaNs and $0 prices\n",
        "df = df.drop_duplicates()\n",
        "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors=\"coerce\")\n",
        "df[\"Kilometres\"] = pd.to_numeric(df[\"Kilometres\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"Price\", \"Kilometres\"])\n",
        "df = df[df[\"Price\"] > 0]\n",
        "print(f\"\\nAfter cleaning, DataFrame shape: {df.shape}\")\n",
        "print(df.head())\n",
        "\n",
        "# Classify columns based on types and relevance\n",
        "quantitative_cols = [\"CylindersinEngine\", \"Doors\", \"Seats\"]\n",
        "qualitative_cols = [\"Brand\", \"Model\", \"Car/Suv\", \"Title\", \"UsedOrNew\", \"Transmission\", \"Engine\", \"DriveType\",\n",
        "                    \"FuelType\", \"ColourExtInt\", \"Location\", \"BodyType\"]\n",
        "continuous_cols = [\"FuelConsumption\", \"Kilometres\", \"Price\", \"Year\"]\n",
        "\n",
        "print(f\"Quantitative Columns: {quantitative_cols}\")\n",
        "print(f\"Qualitative Columns: {qualitative_cols}\")\n",
        "print(f\"Continuous Columns: {continuous_cols}\")\n",
        "\n",
        "# Remove unwanted columns for analysis\n",
        "df_cleaned = df.drop(columns=qualitative_cols)\n",
        "print(\"\\nDataFrame after removing unwanted columns:\")\n",
        "print(df_cleaned.head())\n",
        "print(f\"\\nShape after removing unwanted columns: {df_cleaned.shape}\")\n",
        "\n",
        "# Data types and missing values after cleaning\n",
        "print(\"\\nData types after cleaning:\")\n",
        "print(df_cleaned.dtypes)\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df_cleaned.isnull().sum())\n",
        "\n",
        "#Step 5: Visual Exploratory Data Analysis (EDA) of data (with histogram and barcharts)\n",
        "print(\"\\n### Visual Exploratory Data Analysis ###\")\n",
        "\n",
        "# Plot: Vehicle count by Brand (Bar plot)\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.countplot(x='Brand', data=df, order=df['Brand'].value_counts().index)\n",
        "plt.title('Figure 2: Vehicle Count by Brand', fontsize=16)\n",
        "plt.xlabel('Brand', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# More categorical and continuous variable visualizations (Figure numbers start from 3)\n",
        "categorical_variables = ['UsedOrNew', 'Transmission', 'Engine', 'DriveType', 'FuelType', 'ColourExtInt', 'Location',\n",
        "                         'BodyType']\n",
        "continuous_variables = [\"FuelConsumption\", \"Kilometres\", \"Price\", \"Year\"]\n",
        "\n",
        "\n",
        "# Function for categorical variable plots\n",
        "def create_bar_plot(dataframe, column_name, figure_number, threshold=5):\n",
        "    cleaned_data = dataframe[column_name].dropna()\n",
        "    counts = cleaned_data.value_counts()\n",
        "    combined_counts = pd.concat([counts[counts >= threshold], pd.Series({'Other': counts[counts < threshold].sum()})])\n",
        "    fig = px.bar(x=combined_counts.index, y=combined_counts.values, labels={'x': column_name, 'y': 'Count'},\n",
        "                 title=f'Figure {figure_number}: Count of Vehicles by {column_name}')\n",
        "    fig.update_layout(xaxis_tickangle=-45, height=600)\n",
        "    fig.update_traces(marker_line_color='black', marker_line_width=1.5)\n",
        "    fig.show()\n",
        "\n",
        "# Create bar plots for each categorical variable\n",
        "for idx, var in enumerate(categorical_variables, start=3):\n",
        "    create_bar_plot(df, var, idx)\n",
        "\n",
        "# Function for continuous variable histograms\n",
        "def create_histogram(dataframe, column_name, figure_number, bins=30):\n",
        "    fig = px.histogram(dataframe[column_name].dropna(), x=column_name, nbins=bins,\n",
        "                       title=f'Figure {figure_number}: Distribution of {column_name}')\n",
        "    fig.update_layout(xaxis_title=column_name, yaxis_title='Count', height=600)\n",
        "    fig.update_traces(marker_line_color='black', marker_line_width=1.5)\n",
        "    fig.show()\n",
        "\n",
        "# Create histograms for continuous variables\n",
        "for idx, var in enumerate(continuous_variables, start=11):\n",
        "    create_histogram(df, var, idx)\n",
        "\n",
        "# Step 6: Outlier Analysis\n",
        "print(\"\\n### Outlier Analysis ###\")\n",
        "\n",
        "# Function to detect and remove outliers using the IQR method\n",
        "def remove_outliers_iqr(dataframe, columns):\n",
        "\n",
        "    # Create a copy of the DataFrame to avoid modifying the original\n",
        "    dataframe_cleaned = dataframe.copy()\n",
        "\n",
        "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "    Q1 = columns.quantile(0.25)\n",
        "    Q3 = columns.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define the lower and upper bounds for outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Remove outliers from the DataFrame based on the IQR range\n",
        "    for col in columns.columns:\n",
        "        dataframe_cleaned = dataframe_cleaned[(dataframe_cleaned[col] >= lower_bound[col]) &\n",
        "                                              (dataframe_cleaned[col] <= upper_bound[col])]\n",
        "\n",
        "    # Count the number of outliers per column\n",
        "    outliers = (columns < lower_bound) | (columns > upper_bound)\n",
        "    outlier_count = outliers.sum()\n",
        "\n",
        "    # Create a DataFrame summarizing the number of outliers per column\n",
        "    dtype_dataframe_outlier = pd.DataFrame(outlier_count, columns=[\"Number of Outliers\"]).reset_index()\n",
        "    dtype_dataframe_outlier.rename(columns={\"index\": \"Column Name\"}, inplace=True)\n",
        "\n",
        "    return dtype_dataframe_outlier, dataframe_cleaned\n",
        "\n",
        "# Apply the outlier removal function to the DataFrame\n",
        "selected_numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64'])\n",
        "df_outlier_summary, df_cleaned_without_outliers = remove_outliers_iqr(df_cleaned, selected_numeric_columns)\n",
        "\n",
        "# Print the outlier summary\n",
        "print(f\"\\nOutlier Summary:\\n{df_outlier_summary}\")\n",
        "\n",
        "# Optional: Print the remaining rows after outlier removal\n",
        "print(f\"\\nDataFrame after outlier removal (first 5 rows):\\n{df_cleaned_without_outliers.head()}\")\n",
        "print(f\"Remaining rows after outlier removal: {df_cleaned_without_outliers.shape}\")\n",
        "\n",
        "\n",
        "# Visualize the distribution of a continuous variable before and after outlier removal\n",
        "for var in continuous_variables:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Original data histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(df[var], bins=40, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    plt.title(f'Original Distribution of {var}')\n",
        "    plt.xlabel(var)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Cleaned data histogram\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(df_cleaned[var], bins=40, color='darkorange', edgecolor='black', alpha=0.7)\n",
        "    plt.title(f'Cleaned Distribution of {var} (Outliers Removed)')\n",
        "    plt.xlabel(var)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Step 7: Missing Value Analysis and Treatment Options\n",
        "print(\"\\n### Missing Value Analysis ###\")\n",
        "\n",
        "# Calculate the number of missing values for each column in df_cleaned_without_outliers\n",
        "missing_values = df_cleaned_without_outliers.isnull().sum()\n",
        "\n",
        "# Create a DataFrame to display missing values per column\n",
        "dtype_df_missing_values = pd.DataFrame(missing_values, columns=[\"Missing Values\"]).reset_index()\n",
        "dtype_df_missing_values = dtype_df_missing_values.rename(columns={\"index\": \"Column Name\"})\n",
        "\n",
        "# Display the missing values\n",
        "print(dtype_df_missing_values)\n",
        "\n",
        "# Delete rows with missing values\n",
        "df_cleaned_without_missing = df_cleaned_without_outliers.dropna()\n",
        "print(f\"\\nDataFrame after deleting rows with missing values (first 5 rows): {df_cleaned_without_missing.shape}\")\n",
        "\n",
        "# Impute missing values with MEDIAN for continuous variables\n",
        "continuous_cols = [\"FuelConsumption\", \"Kilometres\", \"Price\", \"Year\"]\n",
        "\n",
        "for col in continuous_cols:\n",
        "    if col in df_cleaned_without_outliers.columns:\n",
        "        if df_cleaned_without_outliers[col].isnull().sum() > 0:\n",
        "            median_value = df_cleaned_without_outliers[col].median()\n",
        "            df_cleaned_without_outliers[col].fillna(median_value, inplace=True)\n",
        "\n",
        "print(f\"\\nDataFrame after imputing continuous variables with MEDIAN (first 5 rows):\\n{df_cleaned_without_outliers.head()}\")\n",
        "\n",
        "# Impute missing values with MODE for categorical variables\n",
        "qualitative_cols = [\"Brand\", \"Model\", \"Car/Suv\", \"Title\", \"UsedOrNew\", \"Transmission\", \"Engine\", \"DriveType\",\n",
        "                    \"FuelType\", \"ColourExtInt\", \"Location\", \"BodyType\"]\n",
        "\n",
        "for col in qualitative_cols:\n",
        "    if col in df_cleaned_without_outliers.columns:\n",
        "        if df_cleaned_without_outliers[col].isnull().sum() > 0:\n",
        "            mode_value = df_cleaned_without_outliers[col].mode()[0]\n",
        "            df_cleaned_without_outliers[col].fillna(mode_value, inplace=True)\n",
        "\n",
        "print(f\"\\nDataFrame after imputing categorical variables with MODE (first 5 rows):\\n{df_cleaned_without_outliers.head()}\")\n",
        "print(\"\\nFinal DataFrame after Missing Value Treatment (first 5 rows)\")\n",
        "print(df_cleaned_without_outliers.head())\n",
        "print(f\"Remaining rows after missing value treatment: {df_cleaned_without_outliers.shape}\")\n",
        "\n",
        "# Step 8: Feature selection - Visual and statistical correlation analysis\n",
        "print(\"\\n## Feature selection - Visual and statistical correlation analysis ##\")\n",
        "\n",
        "def analyze_correlation(data, x_col, y_col, title_prefix):\n",
        "    # Select only numeric columns for analysis\n",
        "    numeric_columns = data[[x_col, y_col]].dropna()\n",
        "    if not numeric_columns.empty:\n",
        "        # Calculate and display the correlation matrix\n",
        "        correlation_matrix = numeric_columns.corr()\n",
        "        print(f\"\\nCorrelation Matrix between {y_col} and {x_col}:\")\n",
        "        print(correlation_matrix)\n",
        "\n",
        "        # Plot the correlation matrix as a heatmap\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=ax)\n",
        "        plt.title(f\"Correlation Matrix: {title_prefix} {x_col} and {y_col}\")  # Adjusted title order\n",
        "        plt.show()\n",
        "\n",
        "        # Extract the correlation value\n",
        "        correlation_value = correlation_matrix.loc[y_col, x_col]\n",
        "        print(f\"\\nPearson Correlation between {y_col} and {x_col}: {correlation_value:.4f}\")\n",
        "\n",
        "        # Interpretation of the correlation strength\n",
        "        if abs(correlation_value) >= 0.7:\n",
        "            print(\"This is a strong correlation.\")\n",
        "        elif abs(correlation_value) >= 0.3:\n",
        "            if correlation_value < 0:\n",
        "                print(\"This is a moderate negative correlation.\")\n",
        "            else:\n",
        "                print(\"This is a moderate positive correlation.\")\n",
        "        else:\n",
        "            if correlation_value < 0:\n",
        "                print(\"This is a weak negative correlation.\")\n",
        "            else:\n",
        "                print(\"This is a weak positive correlation.\")\n",
        "\n",
        "        # Scatter plot with a regression line to visualize the relationship\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.regplot(x=x_col, y=y_col, data=numeric_columns, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n",
        "\n",
        "        # Add the Pearson correlation value to the plot\n",
        "        plt.title(f'Scatter Plot of {y_col} vs {x_col}\\nPearson Correlation: {correlation_value:.4f}', fontsize=16)\n",
        "        plt.xlabel(x_col, fontsize=14)\n",
        "        plt.ylabel(y_col, fontsize=14)\n",
        "\n",
        "        # Adjust x-axis ticks for Year\n",
        "        if x_col == 'Year':\n",
        "            plt.xticks(ticks=numeric_columns[x_col].unique(), rotation=45)\n",
        "\n",
        "        # Show the plot\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No continuous numeric columns available for correlation analysis of {y_col} and {x_col}.\")\n",
        "\n",
        "# Clean and prepare the DataFrame\n",
        "df_cleaned_without_outliers['FuelConsumption'] = df_cleaned_without_outliers['FuelConsumption'].replace(\n",
        "    r'[^0-9.]+', '', regex=True\n",
        ")\n",
        "df_cleaned_without_outliers['FuelConsumption'] = pd.to_numeric(df_cleaned_without_outliers['FuelConsumption'], errors='coerce')\n",
        "df_cleaned_without_outliers.dropna(subset=['FuelConsumption', 'Year'], inplace=True)\n",
        "\n",
        "# Analyze Price vs Kilometres\n",
        "analyze_correlation(df_cleaned_without_outliers, 'Kilometres', 'Price', 'Price and')\n",
        "\n",
        "# Analyze Price vs Fuel Consumption\n",
        "analyze_correlation(df_cleaned_without_outliers, 'FuelConsumption', 'Price', 'Price and')\n",
        "\n",
        "# Analyze Price vs Year\n",
        "analyze_correlation(df_cleaned_without_outliers, 'Year', 'Price', 'Price and')\n",
        "\n",
        "# Box Plot: Vehicle Prices by Brand\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='Brand', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Brand', fontsize=16)\n",
        "plt.xlabel('Brand', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the median price for each Car/SUV type\n",
        "top_30 = df.groupby('Car/Suv')['Price'].median().nlargest(30).index\n",
        "plt.figure(figsize=(16, 10))  # Set figure size\n",
        "\n",
        "# Filter the dataframe to only include the top 20 categories\n",
        "sns.boxplot(x='Car/Suv', y='Price', data=df[df['Car/Suv'].isin(top_30)], order=top_30)\n",
        "plt.title('Box Plot of Vehicle Prices by Top 30 Car/SUV Types', fontsize=16)\n",
        "plt.xlabel('Car/SUV Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "\n",
        "# Rotate x-axis labels to prevent overlap\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "# Ensure the layout is not cut off\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: Vehicle Prices by Used or New status\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='UsedOrNew', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Used or New', fontsize=16)\n",
        "plt.xlabel('Used or New', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: Vehicle Prices by Transmission type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Transmission', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Transmission Type', fontsize=16)\n",
        "plt.xlabel('Transmission Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the median price for each engine type and get the top 30\n",
        "top_30_engines = df.groupby('Engine')['Price'].median().nlargest(30).index\n",
        "plt.figure(figsize=(14, 8))  # Set a larger figure size for better display\n",
        "\n",
        "# Filter the dataframe to only include the top 30 engine types\n",
        "sns.boxplot(x='Engine', y='Price', data=df[df['Engine'].isin(top_30_engines)], order=top_30_engines)\n",
        "plt.title('Box Plot of Vehicle Prices by Top 30 Engine Types', fontsize=16)\n",
        "plt.xlabel('Engine Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "\n",
        "# Rotate x-axis labels to prevent overlap\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "# Ensure the layout is not cut off\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: Vehicle Prices by Drive Type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='DriveType', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Drive Type', fontsize=16)\n",
        "plt.xlabel('Drive Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: Vehicle Prices by Fuel Type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='FuelType', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Fuel Type', fontsize=16)\n",
        "plt.xlabel('Fuel Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the median price for each ColourExtInt and get the top 30\n",
        "top_30_colors = df.groupby('ColourExtInt')['Price'].median().nlargest(30).index\n",
        "plt.figure(figsize=(16, 8))  # Set a larger figure size for better display\n",
        "\n",
        "# Filter the dataframe to only include the top 30 color combinations\n",
        "sns.boxplot(x='ColourExtInt', y='Price', data=df[df['ColourExtInt'].isin(top_30_colors)], order=top_30_colors)\n",
        "plt.title('Box Plot of Vehicle Prices by Top 30 Colour Combinations (Exterior/Interior)', fontsize=16)\n",
        "plt.xlabel('Colour (Exterior/Interior)', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "\n",
        "# Rotate x-axis labels to prevent overlap\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "# Ensure the layout is not cut off\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the median price for each location and get the top 30\n",
        "top_30_locations = df.groupby('Location')['Price'].median().nlargest(30).index\n",
        "plt.figure(figsize=(16, 8))  # Set a larger figure size for better display\n",
        "\n",
        "# Filter the dataframe to only include the top 30 locations\n",
        "sns.boxplot(x='Location', y='Price', data=df[df['Location'].isin(top_30_locations)], order=top_30_locations)\n",
        "plt.title('Box Plot of Vehicle Prices by Top 30 Locations', fontsize=16)\n",
        "plt.xlabel('Location', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "\n",
        "# Rotate x-axis labels to prevent overlap\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "# Ensure the layout is not cut off\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: Vehicle Prices by Transmission type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Transmission', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Transmission Type', fontsize=16)\n",
        "plt.xlabel('Transmission Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: Vehicle Prices by Body Type\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.boxplot(x='BodyType', y='Price', data=df)\n",
        "plt.title('Box Plot of Vehicle Prices by Body Type', fontsize=16)\n",
        "plt.xlabel('Body Type', fontsize=14)\n",
        "plt.ylabel('Price (In Thousands)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 9 - Statistical feature selection (categorical vs. continuous) using ANOVA test\n",
        "print(\"\\n## Statistical feature selection (categorical vs. continuous) using ANOVA test ##\")\n",
        "\n",
        "# Define the target variable and categorical columns\n",
        "target_variable = 'Price'  # Replace with the name of your target variable\n",
        "categorical_columns = ['Brand', 'Model', 'Car/Suv', 'Title', 'UsedOrNew', 'Transmission', 'Engine', 'DriveType', 'FuelType', 'ColourExtInt', 'Location', 'BodyType']\n",
        "\n",
        "# Initialize a list to store ANOVA results\n",
        "anova_results = []\n",
        "\n",
        "# Define a function to perform ANOVA for each categorical variable\n",
        "def perform_anova(df, categorical_column, continuous_column):\n",
        "    \"\"\"\n",
        "    This function performs ANOVA to test if there is any significant relationship\n",
        "    between the categorical variables and the continuous target variable.\n",
        "    H0 (null hypothesis): The independent variable does not have any effect on the dependent variable (Price).\n",
        "    H1 (alternative hypothesis): The independent variable does have an effect on the dependent variable (Price).\n",
        "    \"\"\"\n",
        "\n",
        "    # Group the continuous data by the categorical variable\n",
        "    groups = [df[continuous_column][df[categorical_column] == category] for category in df[categorical_column].unique()]\n",
        "\n",
        "    # Perform ANOVA\n",
        "    anova_result = stats.f_oneway(*groups)\n",
        "\n",
        "    # Return a dictionary with the results\n",
        "    return {\n",
        "        'Feature': categorical_column,\n",
        "        'F-Statistic': anova_result.statistic,\n",
        "        'P-Value': anova_result.pvalue\n",
        "    }\n",
        "\n",
        "# Print the hypothesis\n",
        "print(\"Statistical Feature selection using ANOVA test\")\n",
        "print(\"Null Hypothesis (H₀): The independent variable does not have any effect on the dependent variable (Price)\")\n",
        "print(\"Alternative Hypothesis (H₁): The independent variable does have an effect on the dependent variable (Price)\\n\")\n",
        "\n",
        "# Perform ANOVA for each categorical column\n",
        "for column in categorical_columns:\n",
        "    try:\n",
        "        # Perform ANOVA for the current column and append the result\n",
        "        result = perform_anova(df, column, target_variable)\n",
        "\n",
        "        # Append the result to the list along with an interpretation\n",
        "        p_value = result['P-Value']\n",
        "        interpretation = \"\"\n",
        "\n",
        "        # Interpret the p-value\n",
        "        if p_value < 0.01:\n",
        "            interpretation = \"Strong evidence to reject H0: Significant relationship.\"\n",
        "        elif p_value < 0.05:\n",
        "            interpretation = \"Moderate evidence to reject H0: Likely significant relationship.\"\n",
        "        elif p_value < 0.10:\n",
        "            interpretation = \"Weak evidence to reject H0: Possible relationship.\"\n",
        "        else:\n",
        "            interpretation = \"Fail to reject H0: No significant relationship.\"\n",
        "\n",
        "        # Add the interpretation to the result\n",
        "        result['Interpretation'] = interpretation\n",
        "\n",
        "        # Append the result with interpretation to the list\n",
        "        anova_results.append(result)\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not perform ANOVA for {column}: {e}\")\n",
        "\n",
        "# Convert the results list into a DataFrame\n",
        "anova_results_df = pd.DataFrame(anova_results)\n",
        "\n",
        "# Format the results to display 6 decimal places\n",
        "anova_results_df['F-Statistic'] = anova_results_df['F-Statistic'].apply(lambda x: f\"{x:.6f}\")\n",
        "anova_results_df['P-Value'] = anova_results_df['P-Value'].apply(lambda x: f\"{x:.6f}\")\n",
        "\n",
        "# Display the formatted ANOVA results with interpretation\n",
        "print(\"\\nFormatted ANOVA Results:\")\n",
        "print(anova_results_df[['Feature', 'F-Statistic', 'P-Value', 'Interpretation']])\n",
        "print(\"First few rows of the dataset:\") # Displaying the first few rows of the dataset to understand its structure\n",
        "print(df.head())\n",
        "\n",
        "print(\"Columns in the dataset:\") # Displaying the columns in the DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "Sel_Columns = ['Brand', 'Model', 'UsedOrNew', 'Transmission', 'Engine', 'DriveType', 'FuelType','Kilometres'] # Selecting the final predictor\n",
        "\n",
        "DataFor_ML = df[Sel_Columns] # Creating a new DataFrame with the selected predictors\n",
        "\n",
        "print(\"Data for Machine Learning (Predictors):\") # Displaying  the first few rows of the new DataFrame for verification\n",
        "print(DataFor_ML.head())\n",
        "\n",
        "# Define the target variable\n",
        "target_variable = 'Price'\n",
        "if target_variable in df.columns:\n",
        "    y = df[target_variable]\n",
        "    print(\"\\nTarget Variable:\")\n",
        "    print(y.head())\n",
        "else:\n",
        "    print(f\"Error: Target variable '{target_variable}' not found in the dataset.\")\n",
        "\n",
        "# Saving the final DataFrame for model traning\n",
        "DataFor_ML.to_pickle('DataForML.pkl')\n",
        "\n",
        "\n",
        "# Step 11 : Data conversion to numeric values for machine learning/predictive analysis\n",
        "\n",
        "print(\"Original DataFrame:\") # Displaying the first few rows of the original DataFrame\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nColumn Names:\") # Checking the column names to identify the correct names\n",
        "print(df.columns)\n",
        "\n",
        "# Converting the specified categorical variables to dummy variables\n",
        "categorical_columns = [\n",
        "    'Brand', 'Model', 'Car/Suv', 'Title', 'UsedOrNew',\n",
        "    'Transmission', 'Engine', 'DriveType', 'FuelType',\n",
        "    'ColourExtInt', 'Location', 'BodyType'\n",
        "]\n",
        "\n",
        "# Use get_dummies to convert categorical variables into dummy/indicator variables\n",
        "df_with_dummies = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Print the column names after get_dummies\n",
        "print(\"\\nColumn Names after get_dummies:\")\n",
        "print(df_with_dummies.columns)\n",
        "\n",
        "# As the column 'Engine' had some NaN Values\n",
        "# Clean the 'Engine' column (extract numeric values) if it exists\n",
        "if 'Engine' in df_with_dummies.columns:\n",
        "    def clean_engine_value(value):\n",
        "\n",
        "# Using Try and Except Method\n",
        "        try:\n",
        "            # Extract the first number (assume it is in the format 'X.X L / 100 km' or similar)\n",
        "            return float(value.split()[0])  # Split by space and take the first part\n",
        "        except (ValueError, IndexError):\n",
        "            return np.nan  # Return NaN if it can't be converted\n",
        "\n",
        "    # Apply the cleaning function\n",
        "    df_with_dummies['Engine'] = df_with_dummies['Engine'].apply(clean_engine_value)\n",
        "\n",
        "# Handle NaN values: filling with 0 for numeric columns\n",
        "df_with_dummies.fillna(0, inplace=True)\n",
        "\n",
        "# Converting all boolean columns to 0 and 1\n",
        "bool_columns = df_with_dummies.select_dtypes(include='bool').columns\n",
        "df_with_dummies[bool_columns] = df_with_dummies[bool_columns].astype(int)\n",
        "\n",
        "# If any string format column then Converting it to numeric\n",
        "for col in df_with_dummies.columns:\n",
        "    if df_with_dummies[col].dtype == 'object':\n",
        "        df_with_dummies[col] = pd.to_numeric(df_with_dummies[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Now converting all remaining columns to int dtype\n",
        "df_with_dummies = df_with_dummies.astype(int)\n",
        "\n",
        "# Displaying the first few rows of the modified DataFrame\n",
        "print(\"\\nDataFrame with Dummy Variables (0 and 1):\")\n",
        "print(df_with_dummies.head())\n",
        "\n",
        "# Saving the Modified DataFrame to a new CSV file\n",
        "df_with_dummies.to_csv('modified_file.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# Step 12 : Train/test data split and standardisation/normalisation of data\n",
        "\n",
        "PredictorScaler = MinMaxScaler()  #Choose Min-Max normalization\n",
        "\n",
        "# Storing the fit object\n",
        "PredictorScalerFit = PredictorScaler.fit(X)\n",
        "\n",
        "# Generating the normalized values of X\n",
        "X_normalized = PredictorScalerFit.transform(X)\n",
        "\n",
        "# Split the normalized data into training and testing sets\n",
        "X_training, X_testing, y_training, y_testing = train_test_split(X_normalized, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Displaying the data\n",
        "print(\"Training set shapes:\")\n",
        "print(f'X_train shape: {X_training.shape}')\n",
        "print(f'y_train shape: {y_training.shape}')\n",
        "print(\"Testing set shapes:\")\n",
        "print(f'X_test shape: {X_testing.shape}')\n",
        "print(f'y_test shape: {y_testing.shape}')\n",
        "\n",
        "\n",
        "\n",
        "# Step 13 : Investigating multiple regression algorithms\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning) # Suppressing  FutureWarnings\n",
        "\n",
        "# Sample dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)  # Any features\n",
        "y = 10000 + (500 * X[:, 0]) + (300 * X[:, 1]) + np.random.randn(100) * 100  # In target variable adding randomness to y\n",
        "\n",
        "# Scaling/normalizing data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the models and Comparing the Performance of Each  Machine Learning Model\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42, max_depth=5),\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42, n_estimators=100, max_depth=10),\n",
        "    \"AdaBoost\": AdaBoostRegressor(random_state=42, n_estimators=100),\n",
        "    \"XGBoost\": XGBRegressor(random_state=42, n_estimators=100, max_depth=5)\n",
        "}\n",
        "\n",
        "# Training, Evaluting and Predicting Each Model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train) # Training the model\n",
        "    y_pred = model.predict(X_test) # Making predictions on the test set\n",
        "\n",
        "    # Calculating\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mean_accuracy = 100 - (mean_squared_error(y_test, y_pred, squared=False) / np.mean(y_test) * 100)\n",
        "    median_accuracy = 100 - (median_absolute_error(y_test, y_pred) / np.median(y_test) * 100)\n",
        "    cross_val_accuracies = cross_val_score(model, X_scaled, y, cv=10, scoring='r2')\n",
        "    accuracy_values = (100 * cross_val_accuracies)  # Converting R² to percentage\n",
        "    final_avg_accuracy = np.mean(accuracy_values)\n",
        "\n",
        "    # Printing results for each model\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  R² Value: {r2:.3f}\")\n",
        "    print(f\"  Mean Accuracy on test data: {mean_accuracy:.2f}%\")\n",
        "    print(f\"  Median Accuracy on test data: {median_accuracy:.2f}%\")\n",
        "    print(f\"  Final Average Accuracy: {final_avg_accuracy:.2f}%\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Ploting histogram of predicted values for better understanding\n",
        "    plt.figure()\n",
        "    plt.hist(y_pred, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
        "    plt.title(f'Histogram of Predicted Values: {name}')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.show()\n",
        "\n",
        "# Step 14 : Selection of Best Model\n",
        "\n",
        "# Comparing Accuracies\n",
        "# Keeping the variable empty\n",
        "Best_Model = \"\"\n",
        "\n",
        "# Keeping the best accuracy 0.0\n",
        "Best_Accuracy = 0.0\n",
        "\n",
        "# Compare model accuracies\n",
        "# Linear Regression\n",
        "if 0.50 > Best_Accuracy:\n",
        "    Best_Model = \"Linear Regression\"\n",
        "    Best_Accuracy = 0.50 # Accuracy of Linear Regression\n",
        "\n",
        "# Decision Tree Regressor\n",
        "if -0.34 > Best_Accuracy:\n",
        "    Best_Model = \"Decision Tree Regressor\"\n",
        "    Best_Accuracy = -0.34 # Accuracy of Decision Tree Regressor\n",
        "\n",
        "# Random Forest Regressor\n",
        "if 0.41 > Best_Accuracy:\n",
        "    Best_Model = \"Random Forest Regressor\"\n",
        "    Best_Accuracy = 0.41 # Accuracy of Random Forest Regressor\n",
        "\n",
        "# AdaBoost Regressor\n",
        "if 0.48 > Best_Accuracy:\n",
        "    Best_Model = \"AdaBoost Regressor\"\n",
        "    Best_Accuracy = 0.48 # Accuracy of AdaBoost Regressor\n",
        "\n",
        "# XGBoost Regressor\n",
        "if 0.31 > Best_Accuracy:\n",
        "    Best_Model = \"XGBoost Regressor\"\n",
        "    Best_Accuracy = 0.31 # Accuracy of XGBoost Regressor\n",
        "\n",
        "# Display the best model and its accuracy\n",
        "print(\"The best model is:\", Best_Model, \"with an average accuracy of:\", f\"{Best_Accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# Step 15 : Tkinter\n",
        "# Step 1: Load your complete dataset\n",
        "data = pd.read_csv('AustralianVehiclePrices.csv')\n",
        "\n",
        "# Step 2: Clean the dataset\n",
        "data['Price'] = pd.to_numeric(data['Price'], errors='coerce')\n",
        "data.dropna(subset=['Price'], inplace=True)\n",
        "\n",
        "data['Kilometres'] = pd.to_numeric(data['Kilometres'], errors='coerce')\n",
        "data['Year'] = pd.to_numeric(data['Year'], errors='coerce')\n",
        "\n",
        "data.dropna(subset=['Kilometres', 'Year'], inplace=True)\n",
        "\n",
        "X = data[['Kilometres', 'Year']]\n",
        "y = data['Price']\n",
        "\n",
        "# Step 3: Train the model using all data\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 4: Save the model as a serialized file\n",
        "joblib.dump(model, 'linear_regression_model.pkl')\n",
        "print(\"Model saved as 'linear_regression_model.pkl'\")\n",
        "\n",
        "def predict_vehicle_price(kilometres, year):\n",
        "    \"\"\"Predict the vehicle price based on input kilometres and year.\"\"\"\n",
        "    loaded_model = joblib.load('linear_regression_model.pkl')\n",
        "    input_data = pd.DataFrame([[kilometres, year]], columns=['Kilometres', 'Year'])\n",
        "    predicted_price = loaded_model.predict(input_data)\n",
        "    return predicted_price[0]\n",
        "\n",
        "def on_predict():\n",
        "    \"\"\"Handle the predict button click event.\"\"\"\n",
        "    try:\n",
        "        kilometres = float(kilometres_entry.get())\n",
        "        year = int(year_entry.get())\n",
        "        predicted_price = predict_vehicle_price(kilometres, year)\n",
        "        messagebox.showinfo(\"Prediction\", f'The predicted vehicle price is: ${predicted_price:.2f}')\n",
        "    except ValueError:\n",
        "        messagebox.showerror(\"Input Error\", \"Please enter valid numeric values for Kilometres and Year.\")\n",
        "\n",
        "# Create the main window\n",
        "root = tk.Tk()\n",
        "root.title(\"Vehicle Price Predictor\")\n",
        "\n",
        "# Create input fields\n",
        "tk.Label(root, text=\"Kilometres:\").pack(pady=5)\n",
        "kilometres_entry = tk.Entry(root)\n",
        "kilometres_entry.pack(pady=5)\n",
        "\n",
        "tk.Label(root, text=\"Year:\").pack(pady=5)\n",
        "year_entry = tk.Entry(root)\n",
        "year_entry.pack(pady=5)\n",
        "\n",
        "# Create Predict button\n",
        "predict_button = tk.Button(root, text=\"Predict\", command=on_predict)\n",
        "predict_button.pack(pady=20)\n",
        "\n",
        "# Add a Quit button\n",
        "quit_button = tk.Button(root, text=\"Quit\", command=root.quit)\n",
        "quit_button.pack(pady=5)\n",
        "\n",
        "# Run the application\n",
        "root.mainloop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2I9DNsa59YU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sGCB4iWlZws9Ohm9WQW0yIfqM_ACCevL",
      "authorship_tag": "ABX9TyMmXbbdNYRV+ymTtBfx2BEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}